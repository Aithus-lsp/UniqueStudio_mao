大纲
===

1. 目标：实现对**未知函数**的拟合

2. 途径：通过多层的网络多次对数据处理来相似函数，一般通过损失函数的梯度下降进行优化．一定程度上可以看成是多层感知机或多层逻辑回归一类事物

3. 原理：根据万能近似性质，必存在网络结构以拟合未知函数，其中，较浅的网络对应与较大的广度，同时容易过拟合，较深的网络对应较小的广度，但是训练困难．

4. 优势:以激活函数跳出了线性函数的局限，适用性更好，拓展性更强，但是复杂度急剧提升，易造成过拟合．

5. 学习难点：
    1. 神经网络设计的灵活性衍生出一堆复杂的网络，开发者大多注重效果而不重理论，产生了一些难以理解却又异常又有的算法．
    2. 自动求导，反向传播，链式法则的应用困难．

6. 前景观点：神经网络乃至机器学习不能没有数学理论支持，虽然目前启发式反复叠加，放缩扩大的深度学习取得巨大成就，但是这不是正确方向．

---

具体内容
===

1．前馈神经网络
---

a.前向传播：前馈神经网络名字的由来，已得数据不对原始数据或参数发生影响，亦及无反馈(反向传播是一种训练手段)，前馈的结果是得到目标(拟合函数的结果)．

b.激活函数：加在隐藏层及输出层的非线性函数函数，一般与线性函数一起使用，原则上只要非线性和大多数点可导即可作为激活函数（计算拟合能否实现不予考虑），常见的有整流线性单元，sigmoid,双曲正切函数，一般认为，双曲正切优于sigmoid,同时，一般对大多数模型使用整流线性单元．

c.输出单元：输出单元影响了损失函数的选择，有用于高斯分布的线性单元，用于伯努利输出分布的sigmoid单元，用于Multinoulli输出分布的softmax单元等．

d.损失函数一般使用对数似然加正则化．

e.优化方法：梯度下降

2．反向传播
---

a.计算图:使用节点与计算方法，函数混合以表示层级的整个运算过程

b.链式法则：${( f( g ( x ) ) )' = f'(g(x)) * g'(x)}$

>注意: 矩阵算法可以干脆由此类推,但须在念左乘右乘.

c.可以利用计算图整合神经网络的各层.


3.反向传播具体实现(最基本的,一些内容可以高级化)
---

a.建立节点类,存储前向传播(需建立两个前向传播,一个面向损失函数,一个面向具体输出,这点再考虑正则化时尤为重要)的函数,反向传播时要调用的梯度函数,前向传播时产生的临时数据(用于输入下一个节点).

b.建立计算图类,包含具体计算图(节点连接方式).建立函数以实现反向传播并将的得到的函数储存在相应的字典方便调用,具体,存储对输入的梯度便于向下传播,存储对参数的梯度便于梯度优化,以得到的字典为基础构建训练函数(可内置).建立整合节点函数以实现前向传播的函数.

c.效果或目标:输入计算图构建一个计算图类graph,向graph输入输入层数据,输入训练指令(多组数据可多次输入或优化计算图,此时,必须分离多组数据函数和单组数据函数),训练好参数后可对未知预测.