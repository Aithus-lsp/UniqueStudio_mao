CBOW
===

参考论文: 
[word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)

[Efficient Estimation of Word Representations in
Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

1.概述
---

cbow是一个词向量训练模型, 省略了隐藏层的激活函数, 以得到更加快速的训练过程, 整体目的是得到每个单词的词向量, 即隐藏层的参数, 附加结果是得到一个单词预测的模型, 一般而言, 该模型的正确率可以忽略不计.

2.原理
---

相邻的单词拥有相关性, 即具有共同特征, 故可以用相邻单词的特征评估目标单词. 
模型产生猜想: 李华在做英语的语法填空, 突发奇想想要做个深度学习的NLP模型帮自己做题, 他做了一个三层的模型, 训练正确率0.1%, 不甘心于自己的模型就此荒废, 他便想啊想啊, 突然发现训练模型的参数中想似单词有相似的参数, 此后过程略......

另外的一种思路: 单预测产生多预测

3.具体操作
---

* 数据清洗: 可以使用ntlk库等 
    目的 - > 得到一个去除停止词, 标点, 将单词都化为原型的单词列表, 保持原序.

* 模型训练
    前向传播: 构建预测的模型, 其中, 隐藏层只有线性变换, 输出层有线性变换, softmax函数, 最终输出是一个关于各单词概率的结构(事实上,在损失函数构建之前, 你可以说他是任何东西, 但谁让CBOW如此有效呢)
    反向传播: 构建反向传播的计算图用于训练.

* 测试,评估

4.启示
---

1. 训练模型与最终想要的到的模型可以是分立的.
2. 当不知道如何得到想要的数据的途径的时候(大多是不知道损失函数), 可以尝试使用想要数据于一个相对简单的模型,以得到想要的数据, 总体而言, 所有操作具有相当的灵活性, 一切以数学依赖与具体结果为基础.
3. 灵活性延展: 就算是知道或是已有确定模型及损失函数, 也要考虑有没有更简单的训练模型, 当然, 要保证效果.
4. 再延展: 当参数相似时, 可以使用不同的训练模型和预测模型, 当然, 参数优化的方法又是另外的一环, 譬如可以使用多个感知机训练逻辑回归, 当然, 准确率是另外一回事了.